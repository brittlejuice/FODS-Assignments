{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Ananlysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance vs No. of Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ex_var_a.png\" align='left' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. As we can see 90% of the variance is captured by just 3 principal components as compared to the orignal 6 features of the dataset.\n",
    "#### 2. Principal components are significant as they caputure the variance of a dataset and decrease the amount of features to work with by a significant amount.\n",
    "#### 3. Principal components are effective as they capture the various features of the dataset and convert then into a single feature i.e. it gives us a feature which is better than orignal features for further analysis and use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps Taken :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading the dataset into pandas dataframe for easier analysis and manipulation.\n",
    "#### 2. Performing EDA steps such as checking for null values (No null values in this case) and removing unwanted features, in the case of PCA unwanted features are categorical as they cannot be numerically ananlyzed and may result in unwanted principal components.\n",
    "#### 3. Creating a covariance matrix from the dataset for PCA using the numpy.cov() function.\n",
    "#### 4. Finding the eigenvalues and eigenvectors of the covariance matrix using the numpy.linalg.eig() function.\n",
    "#### 5. Sorting the eigenvalues and eigenvectors in descending order according to the eigenvalues for correct selection of principal components.\n",
    "#### 6. Choosing number of features which explain atleast 90% of the orignal data variance.\n",
    "#### 7. Calculating and plotting the explained variance and cummulative variance to make an informed decision about number of principal components required to sufficiently capture the orignal data.\n",
    "#### 8. Plotting the pair plots of the orignal data and then plotting the selected principal components onto these plots to better understand how principal components capture the orignal data using libraries like seaborn.pairplot() function for pair plots and using matplotlib.pyplot.arrow() function to plot the vectors onto these plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula used for Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cov_mat.png\" align='left' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula used for Explained Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ex_var_for.png\" align='left' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair Plots of the Orignal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pp.png\" align='left' width='1250'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components projected onto the Pair Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pp_pca.png\" align='left' width='1250'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rmse_vs_comp.png\" align='left' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Looking at the above graph we can conclude that models with 3 or 4 principal components perform better than the other models on the given dataset.\n",
    "\n",
    "#### 2. As we can see choosing the appropriate number of components matters as models with less principal components perform as good as or even better than models with more      principal components thus choosing the appropriate ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps Taken :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading the dataset into pandas dataframe for easier analysis and manipulation.\n",
    "#### 2. Performing EDA steps such as checking for null values and removing unwanted features, in the case of PCA unwanted features are categorical as they cannot be numerically ananlyzed and may result in unwanted principal components.\n",
    "#### 3. Droping the null value rows of the dataset as all the null values are in the target feature column, i.e. replacing these values can affect the performance of the models.\n",
    "#### 4. Creating a seperate dataset for pca analysis without the target variable.\n",
    "#### 5. Creating a covariance matrix from the dataset for PCA using the numpy.cov() function.\n",
    "#### 6. Finding the eigenvalues and eigenvectors of the covariance matrix using the numpy.linalg.eig() function.\n",
    "#### 7. Sorting the eigenvalues and eigenvectors in descending order according to the eigenvalues for correct selection of principal components.\n",
    "#### 8. Choosing number of features which explain between 85% and 99% of the orignal data variance.\n",
    "#### 9. Making a function to give the parameters of a linear regression model called fit() using the optimization formula for linear regression.\n",
    "#### 10. Making a function to calculate MSE loss for a regression model.\n",
    "#### 11. Splitting the dataset into training and testing for model training.\n",
    "#### 12. Training linear regression models for all the selections of principal components and storing the MSE and RMSE values for each one of them.\n",
    "#### 13. Plotting the RMSE Errors for each of the models for comparison.\n",
    "#### 14. Predicting the test dataset values using the best model selected from the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula used for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"reg_formula.png\" align='left' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Error Formula Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mse.png\" align='left' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph for Explained and Cummulative Varinace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ex_var.png\" align='left' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE Error vs No. of Components Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rmse_vs_comp.png\" align='left' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
